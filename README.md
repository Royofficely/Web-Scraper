# ğŸ•·ï¸ Officely AI Web Scraper

A powerful, recursive, URL-aware web scraping tool designed to efficiently extract structured content from websites. Ideal for developers, researchers, and data teams needing high-volume, high-quality data collection.

---

## ğŸš€ Features

* ğŸŒ **Recursive URL Crawling** â€“ Traverse and extract content from linked pages.
* ğŸ¯ **Configurable Depth** â€“ Set max depth for recursion to control scope.
* ğŸ” **Smart URL Filtering** â€“ Include/exclude pages by keyword or prefix.
* ğŸ“ **Organized Output** â€“ Saves to structured folders by domain.
* ğŸ›¡ï¸ **Respectful Crawling** â€“ Includes retry logic, backoff, and pacing.
* âš™ï¸ **Highly Configurable** â€“ All logic controlled via `config.py`.
* âœ‚ï¸ **Text Splitting** â€“ Splits long texts for better chunking.
* ğŸš« **Protocol Exclusion** â€“ Skip `mailto:`, `tel:`, `whatsapp:` etc.
* ğŸ” **Robust Retry System** â€“ With backoff and configurable retries.
* ğŸ”€ **Concurrency Control** â€“ Define max requests and per-host limits.
* ğŸ•’ **Request Pacing** â€“ Optional delays to avoid server overload.
* ğŸ¯ **Targeted Extraction** â€“ Focus only on specific divs per page.

---

## ğŸ§ª Example: Targeting Specific Page Sections

Use the `target_divs` setting to extract only specific HTML components, like a title and article body:

```python
"target_divs": {
    "title": {
        "selector": "#main-content > section > div > div.relative... > header",
        "title": "Article Title"
    },
    "description": {
        "selector": "#main-content > section > div > div.relative... > div",
        "title": "Article Description"
    }
}
```

Each entry defines:

* `selector`: a CSS selector
* `title`: the label for output CSV

The scraper will match and extract only those components from the page.

---

## ğŸ›  Installation

```bash
git clone https://github.com/Royofficely/Web-Scraper.git
cd Web-Scraper
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
python agentim.py install
```

---

## â–¶ï¸ Usage

```bash
python agentim.py run
```

---

## âš™ï¸ Configuration (config.py)

```python
config = {
    "domain": "https://www.example.com",
    "include_keywords": ["blog"],
    "exclude_keywords": ["signup", "login"],
    "max_depth": 2,
    "target_divs": {...},
    "start_with": ["https://www.example.com/docs"],
    "split_length": 2000,
    "excluded_protocols": ['mailto:', 'tel:', 'whatsapp:'],
    "max_retries": 5,
    "base_delay": 1,
    "concurrent_requests": 10,
    "connections_per_host": 5,
    "delay_between_requests": 0.5
}
```

---

## ğŸ“¦ Output

Scraped results are saved as CSV with columns:

* `URL`
* `Chunk`
* `Text`
* `Tag` (if `target_divs` used)

---

## ğŸ§© Troubleshooting

* Make sure youâ€™re in the root directory when running.
* Increase `delay_between_requests` if rate-limited.
* Check log output for retries/errors.
* Use `start_with` to limit initial crawl scope.

---

## ğŸ§‘â€ğŸ’» Dev Setup

1. Install as above
2. Make edits in `officely_web_scraper/`
3. Run `agentim.py run` to test locally

---

## ğŸ§± Project Structure

```
.
â”œâ”€â”€ agentim.py
â”œâ”€â”€ officely_web_scraper/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ scan.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ install.sh
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ¤ Contributing

Pull requests are welcome! Please open an issue for any bugs or suggestions.

---

## ğŸ“„ License

MIT License â€¢ See `LICENSE` for details

---

Made with â¤ï¸ by Roy Nativ @ [Officely AI](https://officely.ai)
